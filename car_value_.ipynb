{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "car value .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "httUjyAGbQab",
        "W-N9S-5JbQad",
        "0ZCFmlbObQae"
      ],
      "authorship_tag": "ABX9TyPhufZvGrhu2EFpFmsX/9hj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b02203054/Carvalue/blob/main/car_value_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB2lPmf-bJax"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp-ldLE4tifx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uniKHFSubcgr",
        "outputId": "9de6d020-1cca-4cfe-9c4e-bf76826e5f81"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttkfx5ZVckzY",
        "outputId": "5b0fb722-f3b0-48cc-9ebc-b3b02d14133c"
      },
      "source": [
        "pip install git+git://github.com/scikit-learn/scikit-learn.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/scikit-learn/scikit-learn.git\n",
            "  Cloning git://github.com/scikit-learn/scikit-learn.git to /tmp/pip-req-build-7xstque2\n",
            "  Running command git clone -q git://github.com/scikit-learn/scikit-learn.git /tmp/pip-req-build-7xstque2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.dev0) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.dev0) (1.4.1)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-learn: filename=scikit_learn-1.0.dev0-cp37-cp37m-linux_x86_64.whl size=18492276 sha256=24d7384afbed14fa245a019ac1d9fa97264c6144828970d2cbd98db0e7c3faed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xfa2cz55/wheels/a1/50/0e/316ef2ff8d4cfade292bd20b49efda94727688a153382745a6\n",
            "Successfully built scikit-learn\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-1.0.dev0 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJRapcaPrprg",
        "outputId": "cb455e44-9174-4f41-8c82-ff0ab1ce5229"
      },
      "source": [
        "pip install sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.dev0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW1a-26LbQaG"
      },
      "source": [
        "##  Car Price Prediction - Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VHvc6Y1lGvVU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0p7jkr0bQaO"
      },
      "source": [
        "###  Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHSlvChLbQaP"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re\n",
        "import pprint\n",
        "import joblib\n",
        "from scipy.stats import boxcox\n",
        "from scipy.special import inv_boxcox\n",
        "import sklearn\n",
        "\n",
        "#Python iterators for efficient looping\n",
        "import itertools\n",
        "\n",
        "#Brute force parameter selection\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "#pd.set_option('max_colwidth', 2000)\n",
        "pd.options.display.max_rows = 500\n",
        "pd.options.display.max_columns = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epFQZEOubQaQ"
      },
      "source": [
        "### Import Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "bccMqtlTbQaR",
        "outputId": "70d46f0c-b0b7-4e8b-a382-494f830f666b"
      },
      "source": [
        "#Import data - austin, dallas, houston, and san antonio.  Two snapshots.\n",
        "\n",
        "austin1=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/austin_ford_2019-11-18.json')\n",
        "dallas1=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/dallas_ford_2019-11-18.json')\n",
        "ep1=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/ep_ford_2019-11-18.json')\n",
        "houston1=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/houston_ford_2019-11-18.json')\n",
        "sa1=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/sa_ford_2019_11_18.json')\n",
        "\n",
        "austin2=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/austin_ford_2019-12-29.json')\n",
        "dallas2=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/dallas_ford_2019-12-29.json')\n",
        "ep2=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/ep_ford_2019-12-29.json')\n",
        "houston2=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/houston_ford_2019-12-29.json')\n",
        "sa2=pd.read_json('/content/drive/My Drive/MIT5970/machine learning project - car value/sa_ford_2019_11_18.json')\n",
        "\n",
        "vehicle = pd.DataFrame()\n",
        "for dframe in [austin1, dallas1, ep1, houston1, sa1, austin2, dallas2, ep2, houston2, sa2]:\n",
        "    vehicle = vehicle.append(dframe, ignore_index = True)\n",
        "display(f'vehicle has {len(vehicle)} rows:', vehicle.tail(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f8c8bbca4105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Import data - austin, dallas, houston, and san antonio.  Two snapshots.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maustin1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/MIT5970/machine learning project - car value/austin_ford_2019-11-18.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdallas1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/MIT5970/machine learning project - car value/dallas_ford_2019-11-18.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mep1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/MIT5970/machine learning project - car value/ep_ford_2019-11-18.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m             )\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected object or value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osI2ksxmbQaS"
      },
      "source": [
        "#The vehicle 'Body' is a comma-delimited list.  Join list into a single string.\n",
        "vehicle['Body'] = vehicle['Body'].str.join(',')\n",
        "\n",
        "#Remove whitespace.  Replace NaNs with None.\n",
        "vehicle['SubLoc'] = vehicle['SubLoc'].str.strip()\n",
        "SubLoc_dict={'SubLoc': {np.nan : 'None'}}\n",
        "vehicle.replace(SubLoc_dict, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7xlx1H6bQaS"
      },
      "source": [
        "#Add feature columns and view dataframe\n",
        "print('vehicle length:', len(vehicle))\n",
        "print('vehicle type:', type(vehicle))\n",
        "\n",
        "vehicle.insert(2,'Location',np.nan)\n",
        "vehicle.insert(3,'Year',np.nan)\n",
        "vehicle.insert(4,'Year_in_Title',np.nan)\n",
        "vehicle.insert(5,'Odometer',np.nan)\n",
        "vehicle.insert(6,'RawMake',np.nan)\n",
        "vehicle.insert(7,'Make',np.nan)\n",
        "vehicle.insert(8,'Model',np.nan)\n",
        "vehicle.insert(9,'Trim',np.nan)\n",
        "vehicle.insert(10,'Seller',np.nan)\n",
        "vehicle.insert(11,'Wreck',np.nan)\n",
        "vehicle.insert(12,'Cab',np.nan)\n",
        "vehicle.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aJmVFscbQaV"
      },
      "source": [
        "#Helper functions.\n",
        "\n",
        "def match_regex_patt(df, target_col, regex_patt, no_match_value='None'):\n",
        "    '''Returns regex_patt matches as list.  Case is ignored.'''\n",
        "    matchList=[]\n",
        "    cnt = 0\n",
        "    for idx in df.index:\n",
        "        m = re.search(regex_patt, df.loc[idx, target_col], flags=re.IGNORECASE) #re.search(pattern, string) gets first match\n",
        "        if m is not None:\n",
        "            matchList.append(m.group(1).lower())\n",
        "        else:\n",
        "            cnt+=1\n",
        "            matchList.append(no_match_value)\n",
        "    print(f'## In {target_col}, {cnt} missed matches for regex pattern: {regex_patt}.\\n')\n",
        "    return matchList\n",
        "\n",
        "def findall_regex_patt(df, target_col, regex_patt, max_items, no_match_value='None'):\n",
        "    '''Returns regex_patt matches found left-to-right as list.  Case is ignored.'''\n",
        "    findallList=[]\n",
        "    cnt = 0\n",
        "    for idx in df.index:\n",
        "        allm = re.findall(regex_patt, df.loc[idx, target_col] + \" \", flags=re.IGNORECASE) #return all matches left-to-right       \n",
        "        \n",
        "        if len(allm) > 0:\n",
        "            allm = allm[0:max_items]  #first two items\n",
        "            #allm = sorted(allm)       #alphabetize \n",
        "            oneString = (\"\").join(allm).strip() #convert list to string and remove any spaces.\n",
        "            findallList.append(oneString.lower())\n",
        "        else:\n",
        "            cnt+=1\n",
        "            findallList.append(no_match_value)\n",
        "    print(f'## In {target_col}, {cnt} missed matches for regex pattern: {regex_patt}.\\n')\n",
        "    return findallList\n",
        "\n",
        "def findall_body_regex_patt(df, target_col, regex_patt, max_items, no_match_value='None'):\n",
        "    '''Returns regex_patt matches found left-to-right as list.  Case is ignored.\n",
        "       Each Model included in pattern.'''\n",
        "    findallList=[]\n",
        "    cnt = 0\n",
        "    for idx in df.index:\n",
        "        patt_w_model = df.loc[idx, \"Model\"] + regex_patt  #regex pattern is: Model + Pattern\n",
        "        allm = re.findall(patt_w_model, df.loc[idx, target_col] + \" \", flags=re.IGNORECASE) #return all matches left-to-right\n",
        "        \n",
        "        if len(allm) > 0:\n",
        "            allm = allm[0:2]     #first two items\n",
        "            #allm = sorted(allm)  #alphabetical \n",
        "            oneString = (\"\").join(allm).strip() #convert list to string and remove any spaces.\n",
        "            findallList.append(oneString.lower())\n",
        "        else:\n",
        "            cnt+=1\n",
        "            findallList.append(no_match_value)\n",
        "    print(f'## In {target_col}, {cnt} missed matches for regex pattern: {regex_patt}.\\n')\n",
        "    return findallList\n",
        "\n",
        "\n",
        "#****************************************Target Encoder******************************************\n",
        "\n",
        "def target_encode_categ(encode_cols, train, test, target, encoder):\n",
        "    '''Returns encoded categorical features.  Encoded feature is a blend of\n",
        "    (1) ExpectedVal( trainTarget | FeatureClass) and \n",
        "    (2) \"Prior\": ExpectedVal(Target) over all training data.  \n",
        "    \n",
        "    Sets testTarget to NaN to stop data leakage.\n",
        "    \n",
        "    Encoder smoothing balances Class average vs Prior. Higher smoothing is stronger\n",
        "    regularization.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    train: training data including target Y\n",
        "    test: test data including target Y\n",
        "    target: target Y\n",
        "    encoder: TargetEncoder(cols_to encode, smoothing_float_value).  \n",
        "    \n",
        "    See https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html\n",
        "    for more parameters.'''\n",
        "    \n",
        "    trn=train.copy(); tst=test.copy();\n",
        "    if target in tst.columns:   #Omit any test targets from encoding.  Prevents leakage.\n",
        "        tst[target] = np.nan  \n",
        "    fullX = trn.append(tst)\n",
        "    fullY = fullX.pop(target)       \n",
        "    fullX_enc = encoder.fit_transform(fullX, fullY)\n",
        "    \n",
        "    #Overwrite train/test features with encoded features.\n",
        "    train[encode_cols] = fullX_enc.loc[train.index, encode_cols]\n",
        "    test[encode_cols] = fullX_enc.loc[test.index, encode_cols]\n",
        "    return train, test\n",
        "\n",
        "def multivar_impute(train, test, target):\n",
        "    '''Impute values with regression on numeric columns, excluding\n",
        "    the target. Returns DataFrames.'''\n",
        "    \n",
        "    from sklearn.experimental import enable_iterative_imputer\n",
        "    from sklearn.impute import IterativeImputer\n",
        "    num_cols_ex_targ = train.select_dtypes(exclude='object').columns.tolist()\n",
        "    num_cols_ex_targ.remove(target)\n",
        "    \n",
        "    #Get imputed dfs.\n",
        "    ii = IterativeImputer(max_iter=10, random_state=5)\n",
        "    train_impX = pd.DataFrame(ii.fit_transform(train[num_cols_ex_targ]), index=train.index, columns=num_cols_ex_targ)\n",
        "    test_impX = pd.DataFrame(ii.transform(test[num_cols_ex_targ]), index=test.index, columns=num_cols_ex_targ)\n",
        "\n",
        "    #Overwrite original features with imputed features.\n",
        "    train[num_cols_ex_targ] = train_impX\n",
        "    test[num_cols_ex_targ] = test_impX\n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMwTry24bQaW"
      },
      "source": [
        "#Get VehicleID from URL_Vehicle.    \n",
        "vehIDList = match_regex_patt(df=vehicle, target_col='URL_Vehicle', regex_patt=r'/(\\d+)\\.html', no_match_value='None')    \n",
        "vehicle['VehicleID'] = vehIDList\n",
        "\n",
        "#Get vehicle Location from URL location.craigslist.org\n",
        "vehLocList = match_regex_patt(df=vehicle, target_col='URL_Vehicle', regex_patt='https://(.+)\\.craigslist', no_match_value='None')    \n",
        "vehicle['Location'] = vehLocList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk32nCJdbQaX"
      },
      "source": [
        "pd.set_option('max_colwidth', 80)\n",
        "vehicle.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC6mFSTkbQaY"
      },
      "source": [
        "#Drop incomplete listing 5247.\n",
        "vehicle.drop(index = 5247, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEdhe3OTbQaZ"
      },
      "source": [
        "#Build Year column from AttribDictionary.  Assert numeric format and none missing.\n",
        "yrList=[]\n",
        "for idx in vehicle.index:\n",
        "    m = re.search(r'(\\d+)', vehicle.loc[idx, 'AttribDictionary']['0'])  #re.search(pattern, string) gets first match\n",
        "    yrList.append(m.group(1))  \n",
        "vehicle['Year'] = yrList    \n",
        "vehicle['Year'] = vehicle['Year'].astype('float').astype('int64')\n",
        "\n",
        "#Keep Years 1995 and later.\n",
        "print('Count of vehicles older than 1999 that were dropped:', sum(vehicle['Year'] < 1999))\n",
        "filt = (vehicle['Year'] >= 1995)\n",
        "vehicle = vehicle[filt]\n",
        "\n",
        "assert vehicle['Year'].dtypes == 'int64',  \"Year must be integer.\"\n",
        "assert len(vehicle[vehicle['Year'].isna()]) == 0, \"Year cannot have NAs.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCAQKSLmbQaa"
      },
      "source": [
        "#EXTRACT Odometer & VIN from AttribDictionary.  \n",
        "odomet=[]; vin = []; vinCNT=0;\n",
        "\n",
        "for idx in vehicle.index:\n",
        "    odo_success = 0\n",
        "    vin_success = 0\n",
        "    \n",
        "    for k in vehicle.loc[idx, 'AttribDictionary'].keys(): #For each index, go through dict keys.\n",
        "        od_m = re.search(r'<span>odometer: <b>(\\d+)</b>', vehicle.loc[idx, 'AttribDictionary'][str(k)]) #re.search(pattern, string) gets first match\n",
        "        vin_m = re.search(r'<span>VIN: <b>(\\w+)</b>', vehicle.loc[idx, 'AttribDictionary'][str(k)]) #re.search(pattern, string) gets first match\n",
        "                        \n",
        "        if od_m is not None:\n",
        "            odo_success = 1\n",
        "            odomet.append(od_m.group(1))            \n",
        "        if vin_m is not None:\n",
        "            vin_success = 1\n",
        "            vin.append(vin_m.group(1))          \n",
        "            \n",
        "    if odo_success != 1:\n",
        "        odomet.append(np.nan)        \n",
        "    if vin_success != 1:\n",
        "        vinCNT += 1\n",
        "        vin.append('None'+str(vinCNT))  #Assign unique value.  VIN is missing--not neccesarily duplicate listing.\n",
        "\n",
        "\n",
        "vehicle['Odometer'] = odomet\n",
        "vehicle['Odometer'] = vehicle['Odometer'].astype('float')\n",
        "vehicle['VIN'] = vin\n",
        "\n",
        "#SCRUB Odometer.\n",
        "#Mine Odometer alternative from Body.\n",
        "odomAlt=r'((\\d+,?\\.?\\d+)(K\\s| miles))'\n",
        "odomAltList = match_regex_patt(df=vehicle, target_col='Body', regex_patt=odomAlt, no_match_value=np.nan)    \n",
        "vehicle['RawOdomAlt'] = odomAltList\n",
        "\n",
        "#Clean RawOdomAlt numbers.\n",
        "vehicle['RawOdomAlt'] = vehicle['RawOdomAlt'].str.replace('[Kk]', '000').str.replace(',', '')\\\n",
        "                        .str.replace('.', '').str.replace('miles', '').str.strip()\n",
        "\n",
        "#Set clean numbers less than 1,000 to np.nan.  Mileage this low is likely invalid.\n",
        "vehicle['OdomAlt'] = vehicle['RawOdomAlt'].astype('float')\n",
        "vehicle.loc[vehicle['OdomAlt'] < 1000, 'OdomAlt'] = np.nan\n",
        "\n",
        "#Assign the OdomAlt value to NaN Odometer values.  Drop RawOdomAlt and OdomAlt.\n",
        "vehicle.loc[vehicle['Odometer'].isna(), 'Odometer'] = vehicle.loc[vehicle['Odometer'].isna(), 'OdomAlt']\n",
        "vehicle.drop(['RawOdomAlt', 'OdomAlt'], axis='columns', inplace=True, errors='igonre')\n",
        "\n",
        "#Fix odometer typo.\n",
        "vehicle.loc[9196, 'Odometer'] = vehicle.loc[9196, 'Odometer'] / 1000\n",
        "vehicle.loc[19738, 'Odometer'] = vehicle.loc[19738, 'Odometer'] / 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "httUjyAGbQab"
      },
      "source": [
        "### Seller, Make, Model, Price, CombMPG, Displacement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siZ-TOgLbQab"
      },
      "source": [
        "#Create Seller feature using URL_vehicle.\n",
        "vehicle['Seller']=vehicle['URL_Vehicle'].str.extract(r'(ct[o|d])')\n",
        "vehicle['Seller']=vehicle['Seller'].str.replace('ctd', 'dealer')\n",
        "vehicle['Seller']=vehicle['Seller'].str.replace('cto', 'owner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb0GGl7mbQab"
      },
      "source": [
        "#Extract RawMake col from AttribDictionary in UPPERCASE.\n",
        "rawMakeList=[]\n",
        "for idx in vehicle.index:\n",
        "    m = re.search(r'<b>(.+)</b>', vehicle.loc[idx, 'AttribDictionary']['0']) #re.search(pattern, string) gets first match\n",
        "    if m is not None:\n",
        "        rawMakeList.append(m.group(1))\n",
        "    else:\n",
        "        print('Missing Make/Model index:', idx)\n",
        "        rawMakeList.append('None')\n",
        "                 \n",
        "vehicle['RawMake'] = rawMakeList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocD7IMFubQab"
      },
      "source": [
        "vehicle.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0fAfE_ZbQac"
      },
      "source": [
        "#Populate Make\n",
        "make_regex=r'\\s(ford)'\n",
        "makeList = match_regex_patt(df=vehicle, target_col='RawMake', regex_patt=make_regex, no_match_value='None')    \n",
        "vehicle['Make'] = makeList\n",
        "\n",
        "#Populate Model.\n",
        "model_regex=r'(bronco|buick|caravan|cargo|cmax|c-max|contour|crown|dually|e-150|e150|e-250|e250|e-350|e350|e-450|e450|ecoline|econoline|ecosport|edge|es350|escape|escort|excursion|expedition|explorer|f-150|f-250|f-350|f-450|f-550|f-650|f-750|f-800|f150|f250|f350|f450|f550|f650|f750|f800|fiesta|five|flex|focus|freestar|freestyle|fusion|lcf|lincoln|mustang|national|ranger|raptor|regency|scion|sport|starcraft|t-150|t-250|t-350|taurus|thunderbird|transit|transit-150|transit-250|transit-350|van|windstar)'\n",
        "model_RawMake = match_regex_patt(df=vehicle, target_col='RawMake', regex_patt=model_regex, no_match_value='None')    \n",
        "vehicle['Model'] = model_RawMake\n",
        "\n",
        "#Populate Model.\n",
        "model_Title = match_regex_patt(df=vehicle, target_col='Title', regex_patt=model_regex, no_match_value='None')\n",
        "vehicle['model_Title'] = model_Title\n",
        "vehicle.loc[vehicle['Model'] == 'None', 'Model'] = vehicle.loc[vehicle['Model'] == 'None', 'model_Title']\n",
        "\n",
        "#Drop redundant column\n",
        "vehicle.drop(['model_Title'], axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRaW_BYvbQac"
      },
      "source": [
        "#Scrub Price - remove any $, periods, and spaces.  Note:  ORDER IS IMPORTANT!\n",
        "#No raw string -- use \"\\\" to escape next character.\n",
        "vehicle['Price']=vehicle['Price'].str.replace('\\,', '')\n",
        "vehicle['Price']=vehicle['Price'].str.replace('\\.00', '')\n",
        "vehicle['Price']=vehicle['Price'].str.replace('\\.0', '')\n",
        "vehicle['Price']=vehicle['Price'].str.replace('\\$', '')\n",
        "vehicle['Price']=vehicle['Price'].str.replace('-', '')\n",
        "vehicle['Price']=vehicle['Price'].str.replace(' ', '')\n",
        "vehicle['Price']=vehicle['Price'].str.replace('\\.', 'None')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV6EPqglbQac"
      },
      "source": [
        "vehicle.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79R4ILU8bQac"
      },
      "source": [
        "#Insert CombMPG and displacement from modelYR.  \n",
        "EPA = joblib.load('/content/drive/My Drive/MIT5970/machine learning project - car value/EPA.joblib')\n",
        "\n",
        "#Backup vehicle indices before merge.\n",
        "vehicle_idx_BAK = vehicle.index\n",
        "\n",
        "#Merge data.  NOTE: Trim-level detail is unavailable in EPA data.  So ALL trim levels get the Model median.\n",
        "#For example, a 2017 Mustang GT gets the same displacement as 2017 ecoboost.  More detail will improve the model.\n",
        "vehicle = vehicle.merge(EPA, how='left', left_on=['Model','Year'], right_on=['clean_model','year'])\n",
        "\n",
        "#Drop redundant columns.  Restore original vehicle indices that were reset by merge\n",
        "vehicle.drop(columns=['clean_model', 'year'], inplace=True)\n",
        "vehicle.rename(columns={'comb08' : 'CombMPG'}, inplace=True)\n",
        "vehicle.index = vehicle_idx_BAK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2a5-PQbbQad"
      },
      "source": [
        "vehicle.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-N9S-5JbQad"
      },
      "source": [
        "### Drop bad rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--1z338hbQad"
      },
      "source": [
        "#Drop TRUCKMAX autos that are outside Texas.  \n",
        "vehicle[vehicle['SubLoc'] == '(TRUCKMAX)']\n",
        "drop_indices = vehicle[vehicle['SubLoc'] == '(TRUCKMAX)'].index\n",
        "print(f'Drop {len(drop_indices)} TRUCKMAX listings outside Texas:', drop_indices)\n",
        "vehicle.drop(index = drop_indices, inplace = True, errors = 'ignore')\n",
        "\n",
        "#Make isn't 'ford'.  Drop corresponding rows.\n",
        "filt = (vehicle['Make'] != 'ford')\n",
        "drop_indices = vehicle[filt].index\n",
        "print(f'Drop {len(drop_indices)} non-ford makes:', drop_indices)\n",
        "vehicle.drop(index = drop_indices, inplace = True, errors = 'ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwMD6l9YbQad"
      },
      "source": [
        "#Price is missing, None, or blank.  Drop corresponding rows.\n",
        "filt = (vehicle['Price'].isna()) | (vehicle['Price'] == 'None') | (vehicle['Price'] == '')\n",
        "drop_indices = vehicle[filt].index\n",
        "print(f'## Drop {len(drop_indices)} missing price rows:', drop_indices, '\\n')\n",
        "vehicle.drop(index = drop_indices, inplace = True, errors = 'ignore')\n",
        "\n",
        "#Price is \"NoneNoneNone\"\n",
        "filt = (vehicle['Price'] == 'NoneNoneNone')\n",
        "drop_indices = vehicle[filt].index\n",
        "print(f'## Drop {len(drop_indices)} bad prices:', drop_indices)\n",
        "vehicle.drop(index = drop_indices, inplace = True, errors = 'ignore')\n",
        "\n",
        "#Cast Price as float.  Stop code execution if NaNs exist.\n",
        "vehicle['Price'] = vehicle['Price'].astype('float')\n",
        "assert len(vehicle[vehicle['Price'].isna()]) == 0, \"Price cannot have NAs.\"\n",
        "\n",
        "#Flag invalid \"DOWN/DP\" prices by searching Title/SubLoc.  Drop \"DOWN\" prices under $5,001.  \n",
        "#Cannot be \"DOWNTOWN\".\n",
        "downList=[]\n",
        "for idx in vehicle.index:\n",
        "    m = re.search(r'(DP |DO$|DOWN)[^T]', vehicle.loc[idx, 'Title'] + vehicle.loc[idx, 'Body']\\\n",
        "        + vehicle.loc[idx, 'SubLoc'] + \" \", flags=re.IGNORECASE)\n",
        "    if m is not None:\n",
        "        downList.append('down')\n",
        "    else:\n",
        "        downList.append('None')\n",
        "                 \n",
        "vehicle['DownFlag'] = downList\n",
        "\n",
        "filt = (vehicle['DownFlag'] == 'down') & (vehicle['Price'] < 5001)\n",
        "drop_indices = vehicle[filt].index\n",
        "print(f'## Drop {len(drop_indices)} DOWN prices under $5,001:', drop_indices)\n",
        "vehicle.drop(index = drop_indices, inplace = True, errors = 'ignore')\n",
        "\n",
        "#Drop very high or low prices.\n",
        "filt2 = (vehicle['Price'] > 74999) | (vehicle['Price'] < 501)\n",
        "drop_indices = vehicle[filt2].index\n",
        "print(f'## Drop {len(drop_indices)} prices over $74,999 or under $501:', drop_indices)\n",
        "vehicle.drop(index = drop_indices, inplace = True, errors = 'ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK7d9cJObQae"
      },
      "source": [
        "vehicle.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx7loT4ibQae"
      },
      "source": [
        "#Fix Model names.\n",
        "model_dict={\n",
        "            'Model': {np.nan : 'None', \n",
        "                      '' : 'None',\n",
        "                      'cmax' : 'c-max', 'e150' : 'e-150', 'e250' : 'e-250', 'e350' : 'e-350',\n",
        "                      'f150' : 'f-150', 'f250' : 'f-250', 'f350' : 'f-350', 'f450' : 'f-450',\n",
        "                     }\n",
        "            }\n",
        "\n",
        "vehicle.replace(model_dict, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL85JD3IbQae"
      },
      "source": [
        "#Set Odometer=0 to NaN.\n",
        "odZeroFilt = (vehicle['Odometer'] == 0)\n",
        "zero_indices = vehicle[odZeroFilt].index\n",
        "print(f'# {len(zero_indices)} Odometer values are zero.  Set to NaN.', zero_indices)\n",
        "vehicle.loc[zero_indices, 'Odometer'] = np.nan\n",
        "\n",
        "#Drop duplicate VINs.\n",
        "print('\\n# Duplicate VINs dropped:', vehicle.duplicated(subset=['VIN'], keep='first').sum())\n",
        "vehicle.drop_duplicates(subset=['VIN'], inplace=True)\n",
        "\n",
        "#Drop listings with the same Title/Odometer/Price.\n",
        "print('\\n# Duplicate Vehicles dropped:', vehicle.duplicated(subset=['Title', 'Odometer', 'Price'], keep='first').sum())\n",
        "vehicle.drop_duplicates(subset=['Title', 'Odometer', 'Price'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-I6jVcTbQae"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZCFmlbObQae"
      },
      "source": [
        "### Drop missing and commercial Models that lack EPA data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PuqD2gMEbQaf"
      },
      "source": [
        "#Remove vehicle 'Model' is None.\n",
        "filt = (vehicle['Model'] == 'None')\n",
        "drop_indices = vehicle[filt].index\n",
        "print(f'# Drop {len(drop_indices)} Models that are \"None\":', drop_indices)\n",
        "vehicle = vehicle.drop(index = drop_indices, errors = 'ignore')\n",
        "\n",
        "#Remove f-250, f-350, and f-450 -- no EPA data.  These vehicles are commercial, not personal.\n",
        "filt = (vehicle['Model'] == 'f-250') | (vehicle['Model'] == 'f-350') | (vehicle['Model'] == 'f-450')\n",
        "drop_indices = vehicle[filt].index\n",
        "print(f'# Drop {len(drop_indices)} Models that are \"f-250\", \"f-350\", or \"f-450\":', drop_indices)\n",
        "vehicle = vehicle.drop(index = drop_indices, errors = 'ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORcFRzY-bQaf"
      },
      "source": [
        "## Drop Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBKFHRrDbQaf"
      },
      "source": [
        "# +++++++++++++++++ DROP OUTLIERS  ++++++++++++++++++++++++\n",
        "\n",
        "# ---Train sqrt price residuals.---\n",
        "# 13843 low price due to accident. Listing omits accident.\n",
        "# Drop 3 sigma outliers\n",
        "\n",
        "# ---Test sqrt price residuals.---\n",
        "# No outliers dropped.  Some of the worst outliers are aftermarket-modified vehicles.\n",
        "\n",
        "drop_indices=[13843,15627, 970, 149, 10850, 11624, 17701, 7413, 638, 4878, 1113, 9892, 4870, 2440, 12187, 40, 5843, 5236, 16436, 16433, 10731, 13365, 11575, 15677, 17267, 10975, 6216, 2814, 15390, 9556, 16664, 5109, 1668, 17335, 11946, 12902, 16443, 10962, 8413, 9739, 5722, 182, 969, 12196, 612, 11677, 7266, 10242, 789, 3319, 15182, 6734, 10700, 13711, 1991, 3192, 1210, 5812, 9751, 12317, 14674, 13518, 1303, 6097, 4102, 5797, 7488, 355, 16056, 12915, 7875, 5850, 15227, 15249, 3180, 16719, 13154]\n",
        "print(f'Drop {len(drop_indices)} outliers:', drop_indices)\n",
        "vehicle = vehicle.drop(index = drop_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIgfh_OObQaf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eJMSCErbQag"
      },
      "source": [
        "#Catplot by Year and Price.  Year is effectively a left-skewed histogram.  Looks like Year^2 could be a good feature!\n",
        "sns.set(font_scale=1.2)\n",
        "c = sns.catplot(x='Year',y='Price', data=vehicle, height=8, aspect=1.2, s=6)  #s changes marker size\n",
        "c.set_xticklabels(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp4_RylVbQag"
      },
      "source": [
        "#Year histogram\n",
        "y=sns.distplot(vehicle['Year'], bins=25, kde=True)\n",
        "y.set_title(\"Normalized Year Histogram\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFzu_Je9bQag"
      },
      "source": [
        "vehicle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM463stulw1q"
      },
      "source": [
        "vehicle = vehicle.drop(columns=['URL_Vehicle', 'Title', 'Year_in_Title', 'RawMake','Body','AttribDictionary', 'ImageDictionary','VehicleID','VIN'])\n",
        "\n",
        "vehicle.dtypes\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD7x8ImXqpp1"
      },
      "source": [
        "# For Decision tree "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8MxWlOAJMvg"
      },
      "source": [
        "#basically import everything\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import graphviz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4dYf2e_KZLy"
      },
      "source": [
        "# set up the variables and target\n",
        "vehicle = vehicle.fillna(0)\n",
        "decision_tree_df_x = vehicle[['Year','Odometer','Trim','Wreck','Cab','displ','CombMPG']].values\n",
        "\n",
        "decision_tree_df_y = vehicle[\"Price\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9GBIxs4zCK3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV9kQz6tJhAh"
      },
      "source": [
        "# split the data into training group and test group\n",
        "decision_tree_df_x_trainset, decision_tree_df_x_testset, decision_tree_df_y_trainset, decision_tree_df_y_testset = train_test_split(decision_tree_df_x, decision_tree_df_y, test_size=0.3, random_state=3)\n",
        "print(np.shape(decision_tree_df_x_testset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kTNUq8GKSn5"
      },
      "source": [
        "#set up the depth we want to achieve, since it's a decision \"tree\"\n",
        "CarTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
        "\n",
        "# it shows the default parameters\n",
        "CarTree "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opDI2hoKLoOC"
      },
      "source": [
        "#finally fit the data into the model\n",
        "\n",
        "CarTree.fit(decision_tree_df_x_trainset,decision_tree_df_y_trainset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu-NTBfBLs78"
      },
      "source": [
        "# make the prediction\n",
        "CarPredTree = CarTree.predict(decision_tree_df_x_testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AsHuDLeL1Tp"
      },
      "source": [
        "# compared with prediction and actual datasets to understand how accurate the model is\n",
        "# formula of \"accuracy\" : # of correctly predict datasets in testset / total # datasets in testset \n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(decision_tree_df_y_testset, CarPredTree))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyXVDInf0DbA"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9o7viOb0zkE"
      },
      "source": [
        "vehicle = vehicle.fillna(0)\n",
        "KNN_df_x = vehicle[['Year','Odometer','Trim','Wreck','Cab','displ','CombMPG']].values\n",
        "KNN_df_y = vehicle[\"Price\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8vT4Ovj0Gh1"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import NullFormatter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn import preprocessing\n",
        "%matplotlib inline\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eMI06XU0ZWl"
      },
      "source": [
        "KNN_df_x = preprocessing.StandardScaler().fit(KNN_df_x).transform(KNN_df_x.astype(float))\n",
        "KNN_df_x[0:5]\n",
        "from sklearn.model_selection import train_test_split\n",
        "KNN_df_x_train, KNN_df_x_test, KNN_df_y_train, KNN_df_y_test = train_test_split( KNN_df_x, KNN_df_y, test_size=0.2, random_state=4)\n",
        "print ('Train set:', KNN_df_x_train.shape,  KNN_df_y_train.shape)\n",
        "print ('Test set:', KNN_df_x_test.shape,  KNN_df_y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9FFrbEy0qnR"
      },
      "source": [
        "Ks = 40\n",
        "mean_acc = np.zeros((Ks-1))\n",
        "std_acc = np.zeros((Ks-1))\n",
        "ConfustionMx = [];\n",
        "for n in range(1,Ks):\n",
        "    \n",
        "    #Train Model and Predict  \n",
        "    neigh = KNeighborsClassifier(n_neighbors = n).fit(KNN_df_x_train,KNN_df_y_train)\n",
        "    yhat=neigh.predict(KNN_df_x_test)\n",
        "    mean_acc[n-1] = metrics.accuracy_score(KNN_df_y_test, yhat)\n",
        "\n",
        "    \n",
        "    std_acc[n-1]=np.std(yhat==KNN_df_y_test)/np.sqrt(yhat.shape[0])\n",
        "\n",
        "mean_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX1WzRHu0wsU"
      },
      "source": [
        "plt.plot(range(1,Ks),mean_acc,'g')\n",
        "plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
        "plt.legend(('Accuracy ', '+/- 3xstd'))\n",
        "plt.ylabel('Accuracy ')\n",
        "plt.xlabel('Number of Nabors (K)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}